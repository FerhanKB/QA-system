{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qa-system.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpiR5uO-99HM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "a8f37c40-494d-4786-dbdc-26e18564d4a9"
      },
      "source": [
        "#! pip install wikipedia\n",
        "# In Case installation is needed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=f88a8ea8cfd563ba5b3a074e1cff84ab23b15a3f73d687b7a6af34d5d0f1de27\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-ys2H8bPxG0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2d6124f8-ecb0-45cb-d14d-9367458153f7"
      },
      "source": [
        "import wikipedia\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQIPRSslUDIw"
      },
      "source": [
        "# Query Reformulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ws8j3B_oR2"
      },
      "source": [
        "# Check whether the query given by the user is in one of the four desired form (Who, What, Where, When)\n",
        "def invalid_query(query):\n",
        "  query_words = \"Who What Where When where what when who\".split()\n",
        "  for i in range(len(query_words)):\n",
        "    if (query_words[i] in query):\n",
        "      return False\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDKPxWXho_ez"
      },
      "source": [
        "# If query is not in WH form then get entity tag of query and process on that information\n",
        "def partial_information(query):\n",
        "  labels = []\n",
        "  if (invalid_query(query)==True):\n",
        "    partial = spacy.load('en')\n",
        "    lab = partial(query)\n",
        "    for l in lab.ents:\n",
        "      labels.append(l.label_)\n",
        "    return labels,True\n",
        "  return labels,False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdov3w_2Vk0r"
      },
      "source": [
        "# The Query given by user is processed and Reformulated by chaning the positions of the Non-Stop Words present in the query\n",
        "def QR(query):\n",
        "  tags,part = partial_information(query)\n",
        "  start_words = '? Who What Where When where was what when who'.split()\n",
        "  refined_query = \"\"\n",
        "  # This loop removes the question word from query\n",
        "  for j in range(len(start_words)):\n",
        "    if (j==0):\n",
        "      refined_query = query.replace(start_words[j],'')\n",
        "    else:\n",
        "      refined_query = refined_query.replace(start_words[j],'')\n",
        "  # The remaining part of the query is reformulated to make different rewrites of the given query and also assigned a weight\n",
        "  words = refined_query.split()\n",
        "  new_queries = []\n",
        "  verb = words[0]\n",
        "  for i in range(len(words)):\n",
        "    q = \"\"\n",
        "    k = 1\n",
        "    for j in range(len(words)):\n",
        "      if (i == j):\n",
        "        q += verb + ' '\n",
        "      else:\n",
        "        q += words[k] + ' '\n",
        "        k += 1\n",
        "    new_queries.append([q,5])\n",
        "  q = \"\"\n",
        "  for i in range(1,len(words)):\n",
        "    q += words[i] + ' '\n",
        "  new_queries.append([q,2])\n",
        "\n",
        "  return new_queries,tags,part"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxQOsd4hdwlC"
      },
      "source": [
        "# Ngram Mining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GklcSpse5CE6"
      },
      "source": [
        "# This function takes the reformulated queries and get the summary of the desired query from Wikipedia\n",
        "# The summary is then pre-processed i.e punctuation is removed to make ngrams\n",
        "def information(reformed_queries):\n",
        "  remove_letters = \". ? / ( ) : ! , \\ \\n =\".split()\n",
        "  summary = []\n",
        "  for i in range(len(reformed_queries)):\n",
        "    summary.append(wikipedia.summary(reformed_queries[i][0]))\n",
        "  refined_summary = []\n",
        "  for i in range(len(summary)):\n",
        "    punctuation_removed = []\n",
        "    for j in range(len(remove_letters)):\n",
        "      if (j==0):\n",
        "        punctuation_removed.append(summary[i].replace(remove_letters[j],''))\n",
        "      else:\n",
        "        punctuation_removed.append(punctuation_removed[-1].replace(remove_letters[j],''))\n",
        "    refined_summary.append(punctuation_removed[-1])\n",
        "  return refined_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZT8iSKiBnyT"
      },
      "source": [
        "# This function take the pre-processed summary and tokenize it i.e give all the words in the vocabulary\n",
        "def tokenize(refined_summary):\n",
        "  words = []\n",
        "  for i in range(len(refined_summary)):\n",
        "    words.append(nltk.word_tokenize(refined_summary[i]))\n",
        "  return words\n",
        "\n",
        "# These 3 function compute the unique unigram, bigram and trigram from the tokens of summary\n",
        "def generate_unigram(ngram_token):\n",
        "  unigram = []\n",
        "  c_unigram = []\n",
        "  for i in range(len(ngram_token)):\n",
        "    ug = []\n",
        "    cug = []\n",
        "    for j in range(len(ngram_token[i])):\n",
        "      if (ngram_token[i][j] not in unigram):\n",
        "        ug.append(ngram_token[i][j])\n",
        "        cug.append(1)\n",
        "    unigram.append(ug)\n",
        "    c_unigram.append(cug)\n",
        "  return unigram,c_unigram\n",
        "\n",
        "def generate_bigram(ngram_token):\n",
        "  bigram = []\n",
        "  c_bigram = []\n",
        "  for i in range(len(ngram_token)):\n",
        "    bg = []\n",
        "    cbg = []\n",
        "    for j in range(len(ngram_token[i])-1):\n",
        "      if ((ngram_token[i][j]+' '+ngram_token[i][j+1]) not in bigram):\n",
        "        bg.append(ngram_token[i][j]+' '+ngram_token[i][j+1])\n",
        "        cbg.append(1)\n",
        "    bigram.append(bg)\n",
        "    c_bigram.append(cbg)\n",
        "  return bigram,c_bigram\n",
        "\n",
        "def generate_trigram(ngram_token):\n",
        "  trigram = []\n",
        "  c_trigram = []\n",
        "  for i in range(len(ngram_token)):\n",
        "    tg = []\n",
        "    ctg = []\n",
        "    for j in range(len(ngram_token[i])-2):\n",
        "      if ((ngram_token[i][j]+' '+ngram_token[i][j+1]+' '+ngram_token[i][j+2]) not in trigram):\n",
        "        tg.append(ngram_token[i][j]+' '+ngram_token[i][j+1]+' '+ngram_token[i][j+2])\n",
        "        ctg.append(1)\n",
        "    trigram.append(tg)\n",
        "    c_trigram.append(ctg)\n",
        "  return trigram,c_trigram\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lgoWtDvTcAN"
      },
      "source": [
        "# These 3 functions assign weights on the basis of an ngram present in number of summaries\n",
        "# A ngram that appears in more summaries gets higher weight than a ngram that appears in less summaries\n",
        "def unigram_weights(ug,cug):\n",
        "  for i in range(len(ug)):\n",
        "    for j in range(len(ug[i])):\n",
        "      for k in range(len(ug)):\n",
        "        if (k != i):\n",
        "          if (ug[i][j] in ug[k]):\n",
        "            cug[i][j] += 1\n",
        "  return cug\n",
        "\n",
        "def bigram_weights(bg,cbg):\n",
        "  for i in range(len(bg)):\n",
        "    for j in range(len(bg[i])):\n",
        "      for k in range(len(bg)):\n",
        "        if (k != i):\n",
        "          if (bg[i][j] in bg[k]):\n",
        "            cbg[i][j] += 1\n",
        "  return cbg\n",
        "\n",
        "def trigram_weights(tg,ctg):\n",
        "  for i in range(len(tg)):\n",
        "    for j in range(len(tg[i])):\n",
        "      for k in range(len(tg)):\n",
        "        if (k != i):\n",
        "          if (tg[i][j] in tg[k]):\n",
        "            ctg[i][j] += 1\n",
        "  return ctg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbQP61CkmH8z"
      },
      "source": [
        "## Ngram Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHgn3q3KmHC0"
      },
      "source": [
        "# This function analyze the query and assign a filter type on the basis of query question type\n",
        "def query_analysis(query):\n",
        "  if (\"Who\" in query or \"who\" in query):\n",
        "    return [\"PERSON\",\"NORP\"]\n",
        "  elif (\"When\" in query or \"when\" in query):\n",
        "    return [\"DATE\"]\n",
        "  elif (\"Where\" in query or \"where\" in query):\n",
        "    return [\"GPE\",\"LOC\"]\n",
        "  elif (\"What\" in query or \"what\" in query):\n",
        "    return [\"EVENT\",\"PRODUCT\",\"WORK_OF_ART\",\"CARDINAL\",\"MISC\"]\n",
        "\n",
        "# This function adjust the weight of the ngrams on the basis of the filter type given by above function\n",
        "# For example a Who question query is probably asking for a person so all the ngrams with entity relations as PERSON get weight incremented\n",
        "def weight_adjustment_filtered(filter_type,ug,cug,bg,cbg,tg,ctg):\n",
        "  summary = spacy.load('en')\n",
        "  for i in range(len(ug)):\n",
        "    for j in range(len(ug[i])):\n",
        "      word = summary(ug[i][j])\n",
        "      for w in word.ents:\n",
        "        if (w.label_ in filter_type):\n",
        "          cug[i][j] += 1\n",
        "  \n",
        "  for i in range(len(bg)):\n",
        "    for j in range(len(bg[i])):\n",
        "      word = summary(bg[i][j])\n",
        "      for w in word.ents:\n",
        "        if (w.label_ in filter_type):\n",
        "          cbg[i][j] += 1\n",
        "\n",
        "  for i in range(len(tg)):\n",
        "    for j in range(len(tg[i])):\n",
        "      word = summary(bg[i][j])\n",
        "      for w in word.ents:\n",
        "        if (w.label_ in filter_type):\n",
        "          ctg[i][j] += 1\n",
        "\n",
        "  return cug,cbg,ctg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-PPW6tW27Ha"
      },
      "source": [
        "## Ngram Tiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBEVPsTR3EDu"
      },
      "source": [
        "# This perform the merging of the ngram if they overlap\n",
        "# First the top scoring (highly weighted) ngrams are overlapped and then the second best one only(at the moment)\n",
        "# So if we have max weight 5 only weight 5 and 4 will be checked and merged if possible using trigrams\n",
        "def ngram_tiling(filter,ug,cug,bg,cbg,tg,ctg):\n",
        "  candidate = []\n",
        "  weight = []\n",
        "  n = 0\n",
        "  i = 0\n",
        "  while (i < len(ctg)):\n",
        "    if (n==0):\n",
        "      start = max(ctg[i])\n",
        "      answer = tg[i][ctg[i].index(start)]\n",
        "      w = start\n",
        "    else:\n",
        "      start -= 1\n",
        "    for j in range(len(ctg[i])-1):\n",
        "      if (j!=ctg[i].index(start) and count_tri[i][j]==start):\n",
        "        new_text = nltk.word_tokenize(answer)\n",
        "        if (new_text[-1] in tg[i][j]):\n",
        "          new = nltk.word_tokenize(tg[i][j])\n",
        "          answer += ' '+new[-1]\n",
        "          w = max(ctg[i][j],w)\n",
        "    if (n==1):\n",
        "      candidate.append(answer)\n",
        "      weight.append(w)\n",
        "      n = 0\n",
        "      i += 1\n",
        "    else:\n",
        "      n += 1\n",
        "  \n",
        "  return candidate,weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xohjGG4sGvox"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeSj6seACiZW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "057d8d08-7368-44e9-d6ff-2a4945760724"
      },
      "source": [
        "# Main Function\n",
        "print(\"This is a QA system by YourName. It will try to answer questions that start with Who, What, When or Where. Enter *exit* to leave the program.\")\n",
        "query = input(\"Type query: \")\n",
        "while (query!=\"exit\"):\n",
        "  reformed_queries,ft,partial = QR(query)\n",
        "  if (reformed_queries):\n",
        "    refined_summary = information(reformed_queries)\n",
        "    tokens = tokenize(refined_summary)\n",
        "    unigram,count_uni = generate_unigram(tokens)\n",
        "    bigram,count_bi = generate_bigram(tokens)\n",
        "    trigram,count_tri = generate_trigram(tokens)\n",
        "    count_uni = unigram_weights(unigram,count_uni)\n",
        "    count_bi = bigram_weights(bigram,count_bi)\n",
        "    count_tri = trigram_weights(trigram,count_tri)\n",
        "    if(partial==False):\n",
        "      ft = query_analysis(query)\n",
        "    count_uni,count_bi,count_tri = weight_adjustment_filtered(ft,unigram,count_uni,bigram,count_bi,trigram,count_tri)\n",
        "    cand,w = ngram_tiling(ft,unigram,count_uni,bigram,count_bi,trigram,count_tri)\n",
        "    print(cand[w.index(max(w))])\n",
        "  query = input(\"Type query: \")\n",
        "print(\"Thankyou for using my services.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a QA system by YourName. It will try to answer questions that start with Who, What, When or Where. Enter *exit* to leave the program.\n",
            "Type query: Tom Holland\n",
            "Thomas Stanley Holland born June 1996 is an English actor A graduate of the BRIT School in London he began his acting career on stage in the title role of Billy Elliot\n",
            "Type query: exit\n",
            "Thankyou for using my services.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}